{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle分數：0.84462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參考 https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import data.tokenization\n",
    "import pandas as pd # 引用套件並縮寫為 pd  \n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_word=1000\n",
    "maxlen=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "input_word_ids = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32, name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "clf_output = sequence_output[:, 0, :]\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = data.tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv')  \n",
    "data_test = pd.read_csv('data/test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @thetawniest the out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>m1.94 [01:04 utc]?5km s of volcano hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>the latest: more homes razed by northern calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     our deeds are the reason of this #earthquake m...       1\n",
       "1                forest fire near la ronge sask. canada       1\n",
       "2     all residents asked to 'shelter in place' are ...       1\n",
       "3     13,000 people receive #wildfires evacuation or...       1\n",
       "4     just got sent this photo from ruby #alaska as ...       1\n",
       "...                                                 ...     ...\n",
       "7608  two giant cranes holding a bridge collapse int...       1\n",
       "7609  @aria_ahrary @thetawniest the out of control w...       1\n",
       "7610  m1.94 [01:04 utc]?5km s of volcano hawaii. htt...       1\n",
       "7611  police investigating after an e-bike collided ...       1\n",
       "7612  the latest: more homes razed by northern calif...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=data_train[['text','target']]\n",
    "data_train['text']=data_train['text'].str.lower()\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalypse lighting. #spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>earthquake safety los angeles ûò safety faste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>storm in ri worse than last hurricane. my city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>green line derailment in chicago http://t.co/u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>meg issues hazardous weather outlook (hwo) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>#cityofcalgary has activated its municipal eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                    just happened a terrible car crash\n",
       "1     heard about #earthquake is different cities, s...\n",
       "2     there is a forest fire at spot pond, geese are...\n",
       "3              apocalypse lighting. #spokane #wildfires\n",
       "4         typhoon soudelor kills 28 in china and taiwan\n",
       "...                                                 ...\n",
       "3258  earthquake safety los angeles ûò safety faste...\n",
       "3259  storm in ri worse than last hurricane. my city...\n",
       "3260  green line derailment in chicago http://t.co/u...\n",
       "3261  meg issues hazardous weather outlook (hwo) htt...\n",
       "3262  #cityofcalgary has activated its municipal eme...\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test=data_test[['text']]\n",
    "data_test['text']=data_test['text'].str.lower()\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_seq=[]\n",
    "for text in data_train['text']:\n",
    "    #print(text)\n",
    "    text=tokenizer.tokenize(text)\n",
    "    #print(text)\n",
    "    data_train_seq.append(text)\n",
    "    \n",
    "data_test_seq=[]\n",
    "for text in data_test['text']:\n",
    "    #print(text)\n",
    "    text=tokenizer.tokenize(text)\n",
    "    #print(text)\n",
    "    data_test_seq.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\highs\\anaconda3\\envs\\tensorflow-gpu 36\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\highs\\anaconda3\\envs\\tensorflow-gpu 36\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_train_seq=np.array(data_train_seq)\n",
    "data_test_seq=np.array(data_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', '#', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']),\n",
       "       list(['forest', 'fire', 'near', 'la', 'ron', '##ge', 'sas', '##k', '.', 'canada']),\n",
       "       list(['all', 'residents', 'asked', 'to', \"'\", 'shelter', 'in', 'place', \"'\", 'are', 'being', 'notified', 'by', 'officers', '.', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']),\n",
       "       ...,\n",
       "       list(['m1', '.', '94', '[', '01', ':', '04', 'utc', ']', '?', '5', '##km', 's', 'of', 'volcano', 'hawaii', '.', 'http', ':', '/', '/', 't', '.', 'co', '/', 'z', '##dt', '##oy', '##d', '##8', '##eb', '##j']),\n",
       "       list(['police', 'investigating', 'after', 'an', 'e', '-', 'bike', 'collided', 'with', 'a', 'car', 'in', 'little', 'portugal', '.', 'e', '-', 'bike', 'rider', 'suffered', 'serious', 'non', '-', 'life', 'threatening', 'injuries', '.']),\n",
       "       list(['the', 'latest', ':', 'more', 'homes', 'ra', '##zed', 'by', 'northern', 'california', 'wild', '##fire', '-', 'abc', 'news', 'http', ':', '/', '/', 't', '.', 'co', '/', 'y', '##my', '##4', '##rsk', '##q', '##3d'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t=tokenizer.convert_tokens_to_ids(data_train_seq[0])\n",
    "#print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t2=tokenizer.convert_ids_to_tokens(t)\n",
    "#print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_sentence_to_features(data_train['text'][0],tokenizer,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_features(sentence, tokenizer, max_seq_len):\n",
    "    tokens = ['[CLS]']\n",
    "    tokens.extend(tokenizer.tokenize(sentence))\n",
    "    if len(tokens) > max_seq_len-1:\n",
    "        tokens = tokens[:max_seq_len-1]\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    segment_ids = [0] * len(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    #Zero Mask till seq_length\n",
    "    zero_mask = [0] * (max_seq_len-len(tokens))\n",
    "    input_ids.extend(zero_mask)\n",
    "    input_mask.extend(zero_mask)\n",
    "    segment_ids.extend(zero_mask)\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_sentences_to_features(sentences, tokenizer, max_seq_len=20):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        input_ids, input_mask, segment_ids = convert_sentence_to_features(sentence, tokenizer, max_seq_len)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        all_segment_ids.append(segment_ids)\n",
    "    \n",
    "    return all_input_ids, all_input_mask, all_segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids,train_mask,train_segment_ids=convert_sentences_to_features(data_train['text'],tokenizer,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids,test_mask,test_segment_ids=convert_sentences_to_features(data_test['text'],tokenizer,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=np.array(train_input_ids),np.array(train_mask),np.array(train_segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=np.array(test_input_ids),np.array(test_mask),np.array(test_segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label=data_train['target']\n",
    "train_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in train_input_ids:\n",
    "    if(i[-1]!=0):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 190s 31ms/sample - loss: 0.4348 - accuracy: 0.8100 - val_loss: 0.3701 - val_accuracy: 0.8496\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 185s 30ms/sample - loss: 0.3415 - accuracy: 0.8627 - val_loss: 0.3831 - val_accuracy: 0.8529\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 187s 31ms/sample - loss: 0.2717 - accuracy: 0.8928 - val_loss: 0.4158 - val_accuracy: 0.8339\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 187s 31ms/sample - loss: 0.1951 - accuracy: 0.9258 - val_loss: 0.5285 - val_accuracy: 0.8339\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 188s 31ms/sample - loss: 0.1342 - accuracy: 0.9498 - val_loss: 0.5759 - val_accuracy: 0.8260\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 188s 31ms/sample - loss: 0.1027 - accuracy: 0.9631 - val_loss: 0.6259 - val_accuracy: 0.8418\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 188s 31ms/sample - loss: 0.0979 - accuracy: 0.9629 - val_loss: 0.7171 - val_accuracy: 0.8247\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 188s 31ms/sample - loss: 0.0760 - accuracy: 0.9704 - val_loss: 0.8283 - val_accuracy: 0.8102\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 187s 31ms/sample - loss: 0.0747 - accuracy: 0.9714 - val_loss: 0.8268 - val_accuracy: 0.8247\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 187s 31ms/sample - loss: 0.0702 - accuracy: 0.9714 - val_loss: 1.1039 - val_accuracy: 0.7814\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model_V6_10epochs.h5', monitor='val_loss', save_best_only=True)\n",
    "history = model.fit(\n",
    "    train_input, train_label,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_V6_10epochs.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('Kaggle-6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
